"","mathjax","description","context","plotmath","stamp_function_name","long_description"
"1","$sd_{null}=\\sqrt{\\hat{p}*(1-\\hat{p})*(\\frac{1}{n_1}+\\frac{1}{n_2})}$","Standard error","One-proportion z-test","sd[null] * {phantom() == phantom()} * sqrt(hat(p) ~ symbol('*') ~ (1 - hat(p)) ~ symbol('*') ~ (frac(1, n[1]) * phantom(.) + frac(1, n[2]) * phantom(.)), ) ","stamp_eq_se_z_test","The standard error (SE) of a one proportion z-test is a measure of the variability of the sample proportion estimate around the true population proportion. It tells us how much we expect the sample proportion to vary if we were to repeat the sampling process many times."
"2","$sd_{null}=\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}$ ","Standard Deviation of the null distribution","One-proportion z-test","sd[null] * {phantom() == phantom()} * sqrt(frac(pi[0](1 - pi[0]), n), )*' '","stamp_eq_sd_null_z_test","In a two-proportion z-test, the null distribution is the distribution of the test statistic (z-score) under the assumption that the null hypothesis is true. The standard deviation of this null distribution depends on the sample sizes and the proportion values being compared."
"3","$se=\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$ ","Confidence Interval","One-proportion z-test"," se * {phantom() == phantom()} * sqrt(frac(hat(p)(1 - hat(p)), n), ) ","stamp_eq_ci","The confidence interval is a useful tool in hypothesis testing, as it allows us to estimate the range of possible values for the true population proportion, and can help us decide whether a null hypothesis is plausible or should be rejected.


"
"4","$sd_{null}=s/\\sqrt{n}$","Standard Deviation of the null distribution","One-sample t-test","sd[null] * {phantom() == phantom()} * s / sqrt(n, ) ","stamp_eq_sd_null_t_test",""
"5","$se=s/\sqrt{n}$","Standard Error ","One-sample t-test","se * {phantom() == phantom()} * s / sqrt(n, ) ","stamp_eq_se_t_test",""
"6","$\\textrm{stat}=\\hat{p_1}-\\hat{p_2}$","Unstandardized statistic","One- proportion z-test","plain(stat) * {phantom() == phantom()} * hat(p[1]) - hat(p[2]) ","stamp_statistic",""
"7","$sd_{null}=\\sqrt{\\hat{p}*(1-\\hat{p})*(\\frac{1}{n_1}+\\frac{1}{n_2})}$","Standard Deviation of the null distribution","Two-Proportion z-test","sd[null] * {phantom() == phantom()} * sqrt(hat(p) ~ symbol('*') ~ (1 - hat(p)) ~ symbol('*') ~ (frac(1, n[1]) * phantom(.) + frac(1, n[2]) * phantom(.)), ) ","stamp_sd_null_2_sample_t_test",""
"8","$sd_{null}=\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}$","Standard Deviation of the null distribution","Two-Sample T-test","sd[null] * {phantom() == phantom()} * sqrt(frac(s[1]^{2}, n[1]) * phantom(.) + frac(s[2]^{2}, n[2]), ) ","stamp_sd_null_two_sample_t_test",""
"9","$se=\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}$","Standard Error","Two-Sample T-test","se * {phantom() == phantom()} * sqrt(frac(s[1]^{2}, n[1]) * phantom(.) + frac(s[2]^{2}, n[2]), ) ","stamp_se_two_sample_t_test",""
"10","$E[X] = x_1p_1 + x_2p_2 + x_3p_3 ...x_np_n = \\sum_1^n{x_ip_i}$","Expected Value","Probability Distributions","E * '[' *X * ']' * {phantom() == phantom()} * x[1]*p[1] + x[2]*p[2] + x[3]*p[3]*...x[n]*p[n] * {phantom() == phantom()} * sum(x[i]*p[i], 1, n)","stamp_eq_ev","The expected value of a probability distribution is a measure of the central tendency of the distribution, and represents the average or mean value that we would expect to observe if we were to repeat the random experiment many times. The expected value is a useful statistic because it provides a single number that summarizes the distribution of values that can be observed. It can be used to make predictions about future outcomes, as well as to assess the performance of statistical models or experimental designs.

"
"11","$Var(X) = E[(X-\\mu)^2] = \\sigma^2_x = \\sum_1^n(x_i-\\mu)^2p_i$","Variance","Description of the spread of the Data","Var(X) * {phantom() == phantom()} * E * '[' *(X - mu)^{2} * ']' * {phantom() == phantom()} * sigma[x]^{2} * {phantom() == phantom()} * sum(, 1, n)*(x[i] - mu)^{2}*p[i] ","stamp_eq_var",""
"12","$\\sigma_x = SD(X) = \\sqrt{Var(X)}$","standard deviation","Description of the spread of the Data","sigma[x] * {phantom() == phantom()} * SD(X) * {phantom() == phantom()} * sqrt(Var(X), ) ","stamp_eq_sd",""
"13","$p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }} e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} }$","Normal Distribution","Probability ","p(x) * {phantom() == phantom()} * frac(1, sqrt(2*pi*sigma^{2}, )) * phantom(.)*e^{phantom() - frac((x - mu)^{2}, 2*sigma^{2})} ","stamp_eq_normal","Normal distribution: The normal distribution is one of the most commonly used probability distributions in statistics, and is often used to model continuous variables that are approximately normally distributed. In R, the normal distribution is implemented using the dnorm(), pnorm(), qnorm(), and rnorm() functions."
"14","${{_N}C{_k}} \\cdot p^kq^{N-k}$","Binomial Distribution","Probability "," {{{}[N]}*C * {{}[k]}} %.% p^{k}*q^{N - k}","stamp_eq_binomial","the binomial distribution provides a way to model the probability of observing a specific number of successes in a fixed number of trials, based on the underlying probability of success on each trial. This can be useful for making predictions, testing hypotheses, and assessing the reliability or efficiency of a process. "
"15","$(1-p)^(n-1)*p$","Geometric Distribution","Probability ","(1 - p)^{n - 1} ~ symbol('*') ~ p ","stamp_eq_geometric","In each of these situations, the geometric distribution provides a way to model the probability of observing a specific number of trials until the first success occurs, based on the underlying probability of success on each trial. This can be useful for making predictions, designing experiments or systems, and assessing the reliability or efficiency of a process."
"16","$n!\\(r!*(n-r)!)$","Choose Equation","Probability","{}[n]*C[r] * {phantom() == phantom()} * frac(n*'!', r*'!'(n - r)*'!')","stamp_eq_choose",""
